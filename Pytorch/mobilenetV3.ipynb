{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.mobilenetv3 import MobileNetV3_Large\n",
    "import torch\n",
    "import thop\n",
    "import torchsummary\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV3_Large(num_classes=1000)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bneck: torch.Size([1, 160, 7, 7])\n",
      "inference: 0.4906322956085205 sec\n",
      "out: torch.Size([1, 1000])\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.mobilenetv3.hswish'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.mobilenetv3.Block'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.mobilenetv3.hsigmoid'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.mobilenetv3.SeModule'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.mobilenetv3.MobileNetV3_Large'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "bneck: torch.Size([1, 160, 7, 7])\n",
      "271036064.0 3955916.0\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.rand(1,3,224,224)\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "with torch.no_grad():\n",
    "    out = model(input_tensor)\n",
    "print(\"inference: {} sec\".format(time.time() - st))\n",
    "print(\"out:\", out.shape)\n",
    "\n",
    "flops, params = thop.profile(model=model, inputs=(input_tensor,))\n",
    "print(flops, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bneck: torch.Size([2, 160, 7, 7])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 16, 112, 112]             432\n",
      "       BatchNorm2d-2          [1, 16, 112, 112]              32\n",
      "            hswish-3          [1, 16, 112, 112]               0\n",
      "            Conv2d-4          [1, 16, 112, 112]             256\n",
      "       BatchNorm2d-5          [1, 16, 112, 112]              32\n",
      "              ReLU-6          [1, 16, 112, 112]               0\n",
      "            Conv2d-7          [1, 16, 112, 112]             144\n",
      "       BatchNorm2d-8          [1, 16, 112, 112]              32\n",
      "              ReLU-9          [1, 16, 112, 112]               0\n",
      "           Conv2d-10          [1, 16, 112, 112]             256\n",
      "      BatchNorm2d-11          [1, 16, 112, 112]              32\n",
      "            Block-12          [1, 16, 112, 112]               0\n",
      "           Conv2d-13          [1, 64, 112, 112]           1,024\n",
      "      BatchNorm2d-14          [1, 64, 112, 112]             128\n",
      "             ReLU-15          [1, 64, 112, 112]               0\n",
      "           Conv2d-16            [1, 64, 56, 56]             576\n",
      "      BatchNorm2d-17            [1, 64, 56, 56]             128\n",
      "             ReLU-18            [1, 64, 56, 56]               0\n",
      "           Conv2d-19            [1, 24, 56, 56]           1,536\n",
      "      BatchNorm2d-20            [1, 24, 56, 56]              48\n",
      "            Block-21            [1, 24, 56, 56]               0\n",
      "           Conv2d-22            [1, 72, 56, 56]           1,728\n",
      "      BatchNorm2d-23            [1, 72, 56, 56]             144\n",
      "             ReLU-24            [1, 72, 56, 56]               0\n",
      "           Conv2d-25            [1, 72, 56, 56]             648\n",
      "      BatchNorm2d-26            [1, 72, 56, 56]             144\n",
      "             ReLU-27            [1, 72, 56, 56]               0\n",
      "           Conv2d-28            [1, 24, 56, 56]           1,728\n",
      "      BatchNorm2d-29            [1, 24, 56, 56]              48\n",
      "            Block-30            [1, 24, 56, 56]               0\n",
      "           Conv2d-31            [1, 72, 56, 56]           1,728\n",
      "      BatchNorm2d-32            [1, 72, 56, 56]             144\n",
      "             ReLU-33            [1, 72, 56, 56]               0\n",
      "           Conv2d-34            [1, 72, 28, 28]           1,800\n",
      "      BatchNorm2d-35            [1, 72, 28, 28]             144\n",
      "             ReLU-36            [1, 72, 28, 28]               0\n",
      "           Conv2d-37            [1, 40, 28, 28]           2,880\n",
      "      BatchNorm2d-38            [1, 40, 28, 28]              80\n",
      "           Conv2d-39            [1, 10, 28, 28]             400\n",
      "      BatchNorm2d-40            [1, 10, 28, 28]              20\n",
      "             ReLU-41            [1, 10, 28, 28]               0\n",
      "           Conv2d-42            [1, 40, 28, 28]             400\n",
      "      BatchNorm2d-43            [1, 40, 28, 28]              80\n",
      "         hsigmoid-44            [1, 40, 28, 28]               0\n",
      "         SeModule-45            [1, 40, 28, 28]               0\n",
      "            Block-46            [1, 40, 28, 28]               0\n",
      "           Conv2d-47           [1, 120, 28, 28]           4,800\n",
      "      BatchNorm2d-48           [1, 120, 28, 28]             240\n",
      "             ReLU-49           [1, 120, 28, 28]               0\n",
      "           Conv2d-50           [1, 120, 28, 28]           3,000\n",
      "      BatchNorm2d-51           [1, 120, 28, 28]             240\n",
      "             ReLU-52           [1, 120, 28, 28]               0\n",
      "           Conv2d-53            [1, 40, 28, 28]           4,800\n",
      "      BatchNorm2d-54            [1, 40, 28, 28]              80\n",
      "           Conv2d-55            [1, 10, 28, 28]             400\n",
      "      BatchNorm2d-56            [1, 10, 28, 28]              20\n",
      "             ReLU-57            [1, 10, 28, 28]               0\n",
      "           Conv2d-58            [1, 40, 28, 28]             400\n",
      "      BatchNorm2d-59            [1, 40, 28, 28]              80\n",
      "         hsigmoid-60            [1, 40, 28, 28]               0\n",
      "         SeModule-61            [1, 40, 28, 28]               0\n",
      "            Block-62            [1, 40, 28, 28]               0\n",
      "           Conv2d-63           [1, 120, 28, 28]           4,800\n",
      "      BatchNorm2d-64           [1, 120, 28, 28]             240\n",
      "             ReLU-65           [1, 120, 28, 28]               0\n",
      "           Conv2d-66           [1, 120, 28, 28]           3,000\n",
      "      BatchNorm2d-67           [1, 120, 28, 28]             240\n",
      "             ReLU-68           [1, 120, 28, 28]               0\n",
      "           Conv2d-69            [1, 40, 28, 28]           4,800\n",
      "      BatchNorm2d-70            [1, 40, 28, 28]              80\n",
      "           Conv2d-71            [1, 10, 28, 28]             400\n",
      "      BatchNorm2d-72            [1, 10, 28, 28]              20\n",
      "             ReLU-73            [1, 10, 28, 28]               0\n",
      "           Conv2d-74            [1, 40, 28, 28]             400\n",
      "      BatchNorm2d-75            [1, 40, 28, 28]              80\n",
      "         hsigmoid-76            [1, 40, 28, 28]               0\n",
      "         SeModule-77            [1, 40, 28, 28]               0\n",
      "            Block-78            [1, 40, 28, 28]               0\n",
      "           Conv2d-79           [1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-80           [1, 240, 28, 28]             480\n",
      "           hswish-81           [1, 240, 28, 28]               0\n",
      "           Conv2d-82           [1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-83           [1, 240, 14, 14]             480\n",
      "           hswish-84           [1, 240, 14, 14]               0\n",
      "           Conv2d-85            [1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-86            [1, 80, 14, 14]             160\n",
      "            Block-87            [1, 80, 14, 14]               0\n",
      "           Conv2d-88           [1, 200, 14, 14]          16,000\n",
      "      BatchNorm2d-89           [1, 200, 14, 14]             400\n",
      "           hswish-90           [1, 200, 14, 14]               0\n",
      "           Conv2d-91           [1, 200, 14, 14]           1,800\n",
      "      BatchNorm2d-92           [1, 200, 14, 14]             400\n",
      "           hswish-93           [1, 200, 14, 14]               0\n",
      "           Conv2d-94            [1, 80, 14, 14]          16,000\n",
      "      BatchNorm2d-95            [1, 80, 14, 14]             160\n",
      "            Block-96            [1, 80, 14, 14]               0\n",
      "           Conv2d-97           [1, 184, 14, 14]          14,720\n",
      "      BatchNorm2d-98           [1, 184, 14, 14]             368\n",
      "           hswish-99           [1, 184, 14, 14]               0\n",
      "          Conv2d-100           [1, 184, 14, 14]           1,656\n",
      "     BatchNorm2d-101           [1, 184, 14, 14]             368\n",
      "          hswish-102           [1, 184, 14, 14]               0\n",
      "          Conv2d-103            [1, 80, 14, 14]          14,720\n",
      "     BatchNorm2d-104            [1, 80, 14, 14]             160\n",
      "           Block-105            [1, 80, 14, 14]               0\n",
      "          Conv2d-106           [1, 184, 14, 14]          14,720\n",
      "     BatchNorm2d-107           [1, 184, 14, 14]             368\n",
      "          hswish-108           [1, 184, 14, 14]               0\n",
      "          Conv2d-109           [1, 184, 14, 14]           1,656\n",
      "     BatchNorm2d-110           [1, 184, 14, 14]             368\n",
      "          hswish-111           [1, 184, 14, 14]               0\n",
      "          Conv2d-112            [1, 80, 14, 14]          14,720\n",
      "     BatchNorm2d-113            [1, 80, 14, 14]             160\n",
      "           Block-114            [1, 80, 14, 14]               0\n",
      "          Conv2d-115           [1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-116           [1, 480, 14, 14]             960\n",
      "          hswish-117           [1, 480, 14, 14]               0\n",
      "          Conv2d-118           [1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-119           [1, 480, 14, 14]             960\n",
      "          hswish-120           [1, 480, 14, 14]               0\n",
      "          Conv2d-121           [1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-122           [1, 112, 14, 14]             224\n",
      "          Conv2d-123            [1, 28, 14, 14]           3,136\n",
      "     BatchNorm2d-124            [1, 28, 14, 14]              56\n",
      "            ReLU-125            [1, 28, 14, 14]               0\n",
      "          Conv2d-126           [1, 112, 14, 14]           3,136\n",
      "     BatchNorm2d-127           [1, 112, 14, 14]             224\n",
      "        hsigmoid-128           [1, 112, 14, 14]               0\n",
      "        SeModule-129           [1, 112, 14, 14]               0\n",
      "          Conv2d-130           [1, 112, 14, 14]           8,960\n",
      "     BatchNorm2d-131           [1, 112, 14, 14]             224\n",
      "           Block-132           [1, 112, 14, 14]               0\n",
      "          Conv2d-133           [1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-134           [1, 672, 14, 14]           1,344\n",
      "          hswish-135           [1, 672, 14, 14]               0\n",
      "          Conv2d-136           [1, 672, 14, 14]           6,048\n",
      "     BatchNorm2d-137           [1, 672, 14, 14]           1,344\n",
      "          hswish-138           [1, 672, 14, 14]               0\n",
      "          Conv2d-139           [1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-140           [1, 112, 14, 14]             224\n",
      "          Conv2d-141            [1, 28, 14, 14]           3,136\n",
      "     BatchNorm2d-142            [1, 28, 14, 14]              56\n",
      "            ReLU-143            [1, 28, 14, 14]               0\n",
      "          Conv2d-144           [1, 112, 14, 14]           3,136\n",
      "     BatchNorm2d-145           [1, 112, 14, 14]             224\n",
      "        hsigmoid-146           [1, 112, 14, 14]               0\n",
      "        SeModule-147           [1, 112, 14, 14]               0\n",
      "           Block-148           [1, 112, 14, 14]               0\n",
      "          Conv2d-149           [1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-150           [1, 672, 14, 14]           1,344\n",
      "          hswish-151           [1, 672, 14, 14]               0\n",
      "          Conv2d-152           [1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-153           [1, 672, 14, 14]           1,344\n",
      "          hswish-154           [1, 672, 14, 14]               0\n",
      "          Conv2d-155           [1, 160, 14, 14]         107,520\n",
      "     BatchNorm2d-156           [1, 160, 14, 14]             320\n",
      "          Conv2d-157            [1, 40, 14, 14]           6,400\n",
      "     BatchNorm2d-158            [1, 40, 14, 14]              80\n",
      "            ReLU-159            [1, 40, 14, 14]               0\n",
      "          Conv2d-160           [1, 160, 14, 14]           6,400\n",
      "     BatchNorm2d-161           [1, 160, 14, 14]             320\n",
      "        hsigmoid-162           [1, 160, 14, 14]               0\n",
      "        SeModule-163           [1, 160, 14, 14]               0\n",
      "          Conv2d-164           [1, 160, 14, 14]          17,920\n",
      "     BatchNorm2d-165           [1, 160, 14, 14]             320\n",
      "           Block-166           [1, 160, 14, 14]               0\n",
      "          Conv2d-167           [1, 672, 14, 14]         107,520\n",
      "     BatchNorm2d-168           [1, 672, 14, 14]           1,344\n",
      "          hswish-169           [1, 672, 14, 14]               0\n",
      "          Conv2d-170             [1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-171             [1, 672, 7, 7]           1,344\n",
      "          hswish-172             [1, 672, 7, 7]               0\n",
      "          Conv2d-173             [1, 160, 7, 7]         107,520\n",
      "     BatchNorm2d-174             [1, 160, 7, 7]             320\n",
      "          Conv2d-175              [1, 40, 7, 7]           6,400\n",
      "     BatchNorm2d-176              [1, 40, 7, 7]              80\n",
      "            ReLU-177              [1, 40, 7, 7]               0\n",
      "          Conv2d-178             [1, 160, 7, 7]           6,400\n",
      "     BatchNorm2d-179             [1, 160, 7, 7]             320\n",
      "        hsigmoid-180             [1, 160, 7, 7]               0\n",
      "        SeModule-181             [1, 160, 7, 7]               0\n",
      "           Block-182             [1, 160, 7, 7]               0\n",
      "          Conv2d-183             [1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-184             [1, 960, 7, 7]           1,920\n",
      "          hswish-185             [1, 960, 7, 7]               0\n",
      "          Conv2d-186             [1, 960, 7, 7]          24,000\n",
      "     BatchNorm2d-187             [1, 960, 7, 7]           1,920\n",
      "          hswish-188             [1, 960, 7, 7]               0\n",
      "          Conv2d-189             [1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-190             [1, 160, 7, 7]             320\n",
      "          Conv2d-191              [1, 40, 7, 7]           6,400\n",
      "     BatchNorm2d-192              [1, 40, 7, 7]              80\n",
      "            ReLU-193              [1, 40, 7, 7]               0\n",
      "          Conv2d-194             [1, 160, 7, 7]           6,400\n",
      "     BatchNorm2d-195             [1, 160, 7, 7]             320\n",
      "        hsigmoid-196             [1, 160, 7, 7]               0\n",
      "        SeModule-197             [1, 160, 7, 7]               0\n",
      "           Block-198             [1, 160, 7, 7]               0\n",
      "          Conv2d-199             [1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-200             [1, 960, 7, 7]           1,920\n",
      "          hswish-201             [1, 960, 7, 7]               0\n",
      "          Linear-202                  [1, 1280]       1,230,080\n",
      "     BatchNorm1d-203                  [1, 1280]           2,560\n",
      "          hswish-204                  [1, 1280]               0\n",
      "          Linear-205                  [1, 1000]       1,281,000\n",
      "================================================================\n",
      "Total params: 3,955,916\n",
      "Trainable params: 3,955,916\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 117.27\n",
      "Params size (MB): 15.09\n",
      "Estimated Total Size (MB): 132.93\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_size = tuple(input_tensor.shape[1:])\n",
    "torchsummary.summary(model, input_size=input_size, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, name in enumerate(model.state_dict()):\n",
    "    print(name)\n",
    "    print(name, model.state_dict()[name])\n",
    "    if index > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "torch.Size([16, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_ops\n",
      "total_params\n",
      "0.total_ops\n",
      "0.total_params\n",
      "0.conv1.weight\n",
      "0.conv1.weight ,  torch.Size([16, 16, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for index, name in enumerate(model.bneck.state_dict()):\n",
    "    print(name)\n",
    "#     print(model.bneck.state_dict()[name])\n",
    "    if index > 3:\n",
    "        break\n",
    "\n",
    "for name,param in model.bneck.named_parameters():\n",
    "    print(name, ', ', param.data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bneck.state_dict()['0.total_ops']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
